{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68160f6",
   "metadata": {},
   "source": [
    "# Upload a local datafile to add or replace a Dataset in a Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1ec6e",
   "metadata": {},
   "source": [
    "_\\*\\*The sample code in this notebook limits s3 upload durations to 12 hours. If you think your large file upload may take longer than that, please make use of the `upload_local_datafile` function, whose underlying code supports unlimited upload duration, as seen in the `python/create_dataset_from_local_file.ipynb` notebook.\\*\\*_\n",
    "\n",
    "The script in this notebook performs the upload of a local datafile to a given Collection (as identified by its Collection id), where the datafile becomes a Dataset accessible via the Data Portal UI.\n",
    "\n",
    "In order to use this script, you must...\n",
    "- have a Curation API key (obtained from upper-righthand dropdown in the Data Portal UI after logging in)\n",
    "- know the id of the Collection to which you wish to upload the datafile (taken from `/collections/<collection_id>` in url path in Data Portal UI when viewing the Collection)\n",
    "\n",
    "_For **NEW** Dataset uploads_:\n",
    "- You must create a `dataset_id` to use to uniquely identify the Dataset within its Collection.\n",
    "\n",
    "_For **replacing/updating** existing Datasets_:\n",
    "- Uploads to a `dataset_id` for which there already exists a Dataset in the given Collection will result in the existing Dataset being replaced by the new Dataset created from the datafile that you \n",
    "are uploading.\n",
    "- Alternatively, an existing dataset may be targeted for replacement by using the Dataset's Cellxgene id as the identifier when writing to S3.\n",
    "\n",
    "\n",
    "You can only add/replace Datasets in _private_ Collections or _private revisions_ of published Collections.\n",
    "\n",
    "See examples of _add_ vs _replace_ behavior with different identifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18a750",
   "metadata": {},
   "source": [
    "```\n",
    "identifier = \"new_unused_tag\"\n",
    "# A new Dataset with curator tag 'new_unused_tag' is created from the local datafile and is added to the given Collection\n",
    "\n",
    "identifier = \"existing/Dataset_tag\"\n",
    "# The existing Dataset with curator tag 'existing/Dataset_tag' in the given Collection gets replaced by a new \n",
    "Dataset created from the local datafile\n",
    "\n",
    "identifier = \"abcdef01-2345-6789-abcd-ef01234576789\"\n",
    "# Existing Dataset with id 'abcdef01-2345-6789-abcd-ef01234576789' gets replaced. If no such Dataset exists in the given Collection with this id, no action is taken.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf4dae",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80427063",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"readr\")\n",
    "library(\"aws.s3\")\n",
    "library(\"httr\")\n",
    "library(\"stringr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8964bc",
   "metadata": {},
   "source": [
    "#### <font color='#bc00b0'>Please fill in the required values:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c969a",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Provide the path to your api key file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_file_path <- \"path/to/api-key.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84d52e",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Provide the absolute path to the h5ad datafile to upload</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce19b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename <- \"/absolute/path/to-datafile.h5ad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104aea94",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Enter your chosen `identifier` (see 'identifier' behavior rules in heading above) which will serve as a unique identifier _within this Collection_ for the resultant Dataset.</font>\n",
    "    \n",
    "When using curator tags, we recommmend using a tagging scheme that 1) makes sense to you, and 2) will help organize and facilitate your \n",
    "automation of future uploads for adding new Datasets and replacing existing Datasets. Remember that curator tags can be used as the identifier when _adding or replacing_ Datasets, whereas Dataset id's (uuid's) can only be used as the identifier when _replacing_ Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e9b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier <- \"arbitrary/tag/chosen-by-you\"  # Or \"<dataset_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463870f1",
   "metadata": {},
   "source": [
    "<font color='#bc00b0'>(Required) Enter the id of the Collection to which you wish to add this datafile as a Dataset</font>\n",
    "\n",
    "_The Collection id can be found by looking at the url path in the address bar \n",
    "when viewing your Collection in the UI of the Data Portal website:_ `collections/{collection_id}`_. You can only add/replace Datasets in private Collections or private revisions of published Collections. In order to edit a published Collection, you must first create a revision of that Collection._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86483731",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id <- \"01234567-89ab-cdef-0123-456789abcdef\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de2fe6",
   "metadata": {},
   "source": [
    "### Specify domain (and API url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decaeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_name <- \"cellxgene.cziscience.com\"\n",
    "site_url <- str_interp(\"https://${domain_name}\")\n",
    "api_url_base <- str_interp(\"https://api.${domain_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708156b",
   "metadata": {},
   "source": [
    "### Use API key to obtain a temporary access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key <- read_file(api_key_file_path)\n",
    "access_token_path <- \"/curation/v1/auth/token\"\n",
    "access_token_url <- str_interp(\"${api_url_base}${access_token_path}\")\n",
    "res <- POST(url=access_token_url, add_headers(`x-api-key`=api_key))\n",
    "stop_for_status(res)\n",
    "access_token <- content(res)$access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea089d0e",
   "metadata": {},
   "source": [
    "##### (optional, debug) verify status code of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01dc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res$status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b1f19",
   "metadata": {},
   "source": [
    "### Retrieve temporary s3 write credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_credentials_path <- str_interp(\"/curation/v1/collections/${collection_id}/datasets/s3-upload-credentials\")\n",
    "url <- str_interp(\"${api_url_base}${s3_credentials_path}\")\n",
    "bearer_token <- str_interp(\"Bearer ${access_token}\")\n",
    "res <- GET(url=url, add_headers(`Authorization`=bearer_token, `Content-Type`=\"application/json\"))\n",
    "stop_for_status(res)\n",
    "res_content <- content(res)\n",
    "access_key_id <- res_content$Credentials$AccessKeyId\n",
    "secret_access_key <- res_content$Credentials$SecretAccessKey\n",
    "session_token <- res_content$Credentials$SessionToken\n",
    "upload_path <- res_content$UploadPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14030d8",
   "metadata": {},
   "source": [
    "### Extract formatted upload path from credentials endpoint response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket <- res_content$Bucket\n",
    "key_prefix <- res_content$UploadKeyPrefix\n",
    "upload_key <- paste(key_prefix, identifier, sep=\"\")\n",
    "print(str_interp(\"Full S3 write path is s3://${bucket}/${upload_key}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648e0b2",
   "metadata": {},
   "source": [
    "### Upload file using temporary AWS S3 credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15514bce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Sys.setenv(\n",
    "    \"AWS_ACCESS_KEY_ID\" = access_key_id,\n",
    "    \"AWS_SECRET_ACCESS_KEY\" = secret_access_key,\n",
    "    \"AWS_SESSION_TOKEN\" = session_token,\n",
    "    \"AWS_DEFAULT_REGION\" = \"us-west-2\"\n",
    ")\n",
    "put_object(file=filename, object=upload_key, bucket=bucket)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
